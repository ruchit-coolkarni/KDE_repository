{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bee9f0-a51a-42a5-91dd-8ba1197efd4c",
   "metadata": {},
   "source": [
    "# Step 1: Selection of Station Datasets\n",
    "After selecting stations based on the criterion you have decided, the station dataset files, initially in .txt format, are converted to .parquet format for faster processing compared to .csv. The code below performs this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5cca9-6269-4998-b829-8e8920300f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directories\n",
    "input_dir = ''\n",
    "output_dir = ''\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "#This step could be optional for you depending on the name of your station dataset text file\n",
    "# Function to extract station number from the filename and remove leading zero\n",
    "def extract_station_number(filename):\n",
    "    parts = filename.split('_')\n",
    "    station_number = parts[2]  # Extracts the station number (3rd part of the filename)\n",
    "    return station_number.lstrip('0')  # Removes leading zeros (e.g., '049043' -> '49043')\n",
    "\n",
    "# Column names based on your provided text file\n",
    "columns = []\n",
    "\n",
    "# Loop through all files in the input directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        station_number = extract_station_number(file_name)\n",
    "        \n",
    "        # Construct the full path to the text file\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        try:\n",
    "            # Load the text file into a DataFrame\n",
    "            df = pd.read_csv(file_path, delimiter=',')\n",
    "        \n",
    "            # Save the DataFrame to a Parquet file in the output directory\n",
    "            output_file = os.path.join(output_dir, f\"{station_number}.parquet\")\n",
    "            df.to_parquet(output_file, index=False)\n",
    "        \n",
    "            print(f\"Converted {file_name} to {station_number}.parquet\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Skipped {file_name} due to encoding errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe220a-fde2-43bd-9532-6ec06c92fb38",
   "metadata": {},
   "source": [
    "# Step 2: Applying Quality Control Checks on Rainfall Values\n",
    "Please refer to the Supplementary Figure S1 for detailed description on the Quality Control Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89321a-87f6-4d39-9d8f-47f221dc7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the Parquet files\n",
    "parquet_dir = ''\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = ''\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize counters\n",
    "total_accepted = 0\n",
    "total_non_accepted = 0\n",
    "\n",
    "# Loop through all Parquet files in the directory\n",
    "for filename in os.listdir(parquet_dir):\n",
    "    if filename.endswith('.parquet'):\n",
    "        file_path = os.path.join(parquet_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            # Load the Parquet file into a DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Replace empty strings and spaces with NaN\n",
    "            df.replace({\"\": pd.NA, \" \": pd.NA}, inplace=True)\n",
    "\n",
    "            # Check if required columns exist\n",
    "            required_columns = [\n",
    "                'Quality of precipitation value', 'Year', 'Rainfall',\n",
    "                'Number of days of rain within the days of accumulation',\n",
    "                'Accumulated number of days over which the precipitation was measured'\n",
    "            ]\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                print(f\"Skipping {filename}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Count total rainfall records (non-NaN rainfall)\n",
    "            total_rainfall_records = df['Rainfall'].notna().sum()\n",
    "\n",
    "            # Quality Check 1: Rainfall not NaN, Quality not 'Y', consider if Year > 2019, exclude if Quality is NaN\n",
    "            condition1 = (\n",
    "                (~df['Rainfall'].isna()) &\n",
    "                (df['Quality of precipitation value'] != 'Y') &\n",
    "                (df['Year'] <= 2019) &\n",
    "                (~df['Quality of precipitation value'].isna())\n",
    "            )\n",
    "\n",
    "            # Quality Check 2: Number of days of rain within accumulation must be 1\n",
    "            condition2 = (\n",
    "                (df['Number of days of rain within the days of accumulation'] != 1) &\n",
    "                (~df['Number of days of rain within the days of accumulation'].isna())\n",
    "            )\n",
    "\n",
    "            # Quality Check 3: Accumulated number of days must be 1\n",
    "            condition3 = (\n",
    "                (df['Accumulated number of days over which the precipitation was measured'] != 1) &\n",
    "                (~df['Accumulated number of days over which the precipitation was measured'].isna())\n",
    "            )\n",
    "\n",
    "            # Identify discrepancies (non-accepted records)\n",
    "            discrepancies = df[condition1 | condition2 | condition3]\n",
    "\n",
    "            # Update Rainfall to -99.99 for discrepant rows\n",
    "            df.loc[discrepancies.index, 'Rainfall'] = -99.99\n",
    "\n",
    "            # Count accepted and non-accepted for this file\n",
    "            non_accepted = len(discrepancies)\n",
    "            accepted = total_rainfall_records - non_accepted  # Accepted = total - non-accepted\n",
    "\n",
    "            # Update global counters\n",
    "            total_accepted += accepted\n",
    "            total_non_accepted += non_accepted\n",
    "\n",
    "            # Save the updated DataFrame\n",
    "            output_file = os.path.join(output_dir, filename)\n",
    "            df.to_parquet(output_file, index=False)\n",
    "\n",
    "            print(f\"Processed {filename}: Accepted = {accepted}, Non-accepted = {non_accepted}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Print total accepted and non-accepted records\n",
    "print(f\"\\nTotal rainfall records accepted: {total_accepted}\")\n",
    "print(f\"Total rainfall records non-accepted: {total_non_accepted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901ddab-b437-4f61-838f-d13bec7c9cc4",
   "metadata": {},
   "source": [
    "# Step 3A: Calculating 99th percentile threshold map of New South Wales state with the help of AGCD dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f07930-2c26-4028-9b5c-f076dcc7f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "xr.set_options(keep_attrs=True, display_expand_data=False)\n",
    "np.set_printoptions(threshold=10, edgeitems=2)\n",
    "\n",
    "%xmode minimal\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(memory_limit=None, threads_per_worker=1, n_workers=10)\n",
    "Client()\n",
    "\n",
    "def _preprocess(ds):\n",
    "    return ds.precip.sel(lat=slice(-38,-27),lon=slice(140,154))\n",
    "\n",
    "#Opening the file\n",
    "dr=xr.open_mfdataset('/agcd_v1-0-1_precip_total_r001_daily_*.nc',\n",
    "                     parallel=True, preprocess=_preprocess)\n",
    "\n",
    "dr = dr.chunk({'time':-1, 'lat':25, 'lon':25})\n",
    "\n",
    "\n",
    "\n",
    "def dask_percentile(array: np.ndarray, axis: str, q: float):\n",
    "    '''\n",
    "    Applies np.percentile in dask across an axis\n",
    "    Parameters:\n",
    "    -----------\n",
    "    array: the data to apply along\n",
    "    axis: the dimension to be applied along\n",
    "    q: the percentile\n",
    "   \n",
    "    Returns:\n",
    "    --------\n",
    "    qth percentile of array along axis\n",
    "   \n",
    "    Example\n",
    "    -------\n",
    "    xr.Dataset.data.reduce(xca.dask_percentile,dim='time', q=90)\n",
    "    '''\n",
    "    array = array.rechunk({axis: -1})\n",
    "    return array.map_blocks(\n",
    "        np.nanpercentile,\n",
    "        axis=axis,\n",
    "        q=q,\n",
    "        dtype=array.dtype,\n",
    "        drop_axis=axis)\n",
    "\n",
    "dr_masked=dr.where(dr>=1)\n",
    "\n",
    "#Calculating 99 percentile\n",
    "dr_99=dr_masked.reduce(dask_percentile, q=99,dim='time')\n",
    "dr_99\n",
    "\n",
    "\n",
    "dr_99 = dr_99.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c998f-8b6e-46d1-ac0e-85fb2060143c",
   "metadata": {},
   "source": [
    "# Step 3B: Extracting 99th Percentile threshold for each chosen station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5298e-d2f5-442f-8716-663ffbf9343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = ''\n",
    "sheet_name = ''\n",
    "# Load the data from the Excel file\n",
    "stations_df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Load the NetCDF file\n",
    "dr = xr.open_dataset('/rain_perc99.nc')\n",
    "prcp = dr.precip_99th\n",
    "\n",
    "# Create a list to store the 99th percentile precipitation values\n",
    "r1_99_values = []\n",
    "\n",
    "# Loop through each station in the DataFrame\n",
    "for index, row in stations_df.iterrows():\n",
    "    lat = row['Lat']\n",
    "    lon = row['Lon']\n",
    "    # Select the nearest value for the given coordinates\n",
    "    x = prcp.sel(lat=lat, lon=lon, method='nearest')\n",
    "    # Append the value to the list\n",
    "    r1_99_values.append(x.values)\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "stations_df['R1_99'] = r1_99_values\n",
    "\n",
    "# Save the updated DataFrame back to the same Excel sheet without creating a new sheet\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "    # Write the updated DataFrame back to the same sheet\n",
    "    stations_df.to_excel(writer, index=False, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317990bd-61d5-4294-99fa-f1313536aaa6",
   "metadata": {},
   "source": [
    "# Step 4A: Code to get a list of dates from the accepted rainfall records for each station where the rainfall exceeds its 99th percentile value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb2998-bc17-4588-8170-b6199d52b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories and file paths\n",
    "parquet_dir = ''\n",
    "\n",
    "#This file would contain the information of 99th percentile threshold for each and every chosen station\n",
    "excel_file = ''\n",
    "\n",
    "output_dir = ''\n",
    "\n",
    "# Read the Excel file (Sheet1) to extract station numbers and corresponding R1_99 values\n",
    "station_data = pd.read_excel(excel_file, sheet_name='Sheet1')\n",
    "station_data = station_data[['Station Number', 'R1_99']]\n",
    "\n",
    "# Function to process each Parquet file\n",
    "def process_csv(parquet_file):\n",
    "    # Extract the station number from the filename by removing the .csv extension\n",
    "    filename = os.path.basename(parquet_file)\n",
    "    station_number_str = filename.replace('.parquet', '')\n",
    "    \n",
    "    try:\n",
    "        station_number = int(station_number_str)\n",
    "    except ValueError:\n",
    "        print(f\"Filename '{filename}' does not correspond to a valid station number.\")\n",
    "        return\n",
    "\n",
    "    # Find the R1_99 value for this station number from the Excel data\n",
    "    station_info = station_data[station_data['Station Number'] == station_number]\n",
    "    \n",
    "    if station_info.empty:\n",
    "        print(f\"No R1_99 value found for station {station_number}\")\n",
    "        return\n",
    "    \n",
    "    R1_99_value = station_info['R1_99'].values[0]\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Convert 'Rainfall' column to numeric (in case it contains invalid data)\n",
    "    df['Rainfall'] = pd.to_numeric(df['Rainfall'], errors='coerce')\n",
    "    \n",
    "    # Filter rows where 'Rainfall' exceeds R1_99 value\n",
    "    filtered_df = df[df['Rainfall'] > R1_99_value]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        # Save the filtered rows to a new CSV file in the output directory\n",
    "        output_file = os.path.join(output_dir, f\"{station_number}.parquet\")\n",
    "        filtered_df.to_parquet(output_file, index=False)\n",
    "        print(f\"Saved filtered data for station {station_number} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"No extreme rainfall days found for station {station_number}\")\n",
    "\n",
    "# Process all CSV files in the specified directory\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for file in os.listdir(csv_dir):\n",
    "    if file.endswith('.parquet'):\n",
    "        csv_file_path = os.path.join(csv_dir, file)\n",
    "        process_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e625f-ddc7-4af9-9124-9bdda1fc7d0a",
   "metadata": {},
   "source": [
    "# Step 4B: Getting the list of Extreme Rainfall Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bac9fe-1cf7-4d47-8923-d2e17e445242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the directories containing the parquet files\n",
    "directory1 = \"\"\n",
    "\n",
    "# Initialize dictionaries to count the number of stations recording rainfall on each day\n",
    "rainfall_count = {}\n",
    "rainfall_files = {}\n",
    "\n",
    "# Function to process each directory for a given year\n",
    "def process_directory(directory, input_year):\n",
    "    files_with_data_count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".parquet\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df = df[['Year', 'Month', 'Day', 'Rainfall']]\n",
    "            df_filtered = df[(df['Year'] == input_year) & (df['Rainfall'] > 0)]  # Filter days with rainfall > 0\n",
    "            if not df_filtered.empty:\n",
    "                files_with_data_count += 1\n",
    "            for _, row in df_filtered.iterrows():\n",
    "                day = int(row['Day'])\n",
    "                month = int(row['Month'])\n",
    "                date = datetime(input_year, month, day).strftime(\"%Y-%m-%d\")\n",
    "                if date not in rainfall_count:\n",
    "                    rainfall_count[date] = 0\n",
    "                    rainfall_files[date] = []\n",
    "                rainfall_count[date] += 1\n",
    "                rainfall_files[date].append(filename)\n",
    "    return files_with_data_count\n",
    "\n",
    "# Iterate over the year range from 1858 to 2024\n",
    "for year in range(1858, 2025):\n",
    "    print(f\"Processing year: {year}\")\n",
    "    process_directory(directory1, year)\n",
    "\n",
    "# Create a DataFrame with all dates in the specified range\n",
    "date_range = pd.date_range(start='1858-01-01', end='2024-12-31')\n",
    "rainfall_df = pd.DataFrame(date_range, columns=['Date'])\n",
    "\n",
    "# Include all dates where at least one station recorded rainfall\n",
    "rainfall_df['Station Count'] = rainfall_df['Date'].apply(lambda x: rainfall_count.get(x.strftime(\"%Y-%m-%d\"), 0))\n",
    "\n",
    "# Filter dates where at least one station recorded rainfall (Station Count > 0)\n",
    "valid_dates = rainfall_df[rainfall_df['Station Count'] > 0]\n",
    "\n",
    "# Convert the 'Date' column to string format to avoid formatting issues in Excel\n",
    "valid_dates['Date'] = valid_dates['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# File path to save the data\n",
    "output_file = \"\"\n",
    "\n",
    "# Append the data to the existing Excel file without overwriting\n",
    "try:\n",
    "    with pd.ExcelWriter(output_file, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "        existing_data = pd.read_excel(output_file)  # Read the existing data\n",
    "        new_data = pd.concat([existing_data, valid_dates[['Date', 'Station Count']]])  # Concatenate new data\n",
    "        new_data.to_excel(writer, index=False, sheet_name='Sheet1')  # Write back to the same sheet\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a new one\n",
    "    valid_dates[['Date', 'Station Count']].to_excel(output_file, index=False)\n",
    "\n",
    "# Print the dates where at least one station recorded rainfall\n",
    "print(\"Dates where at least one station recorded rainfall:\")\n",
    "print(valid_dates[['Date', 'Station Count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14353caf-46bf-4cd7-8c72-088a93d4b15b",
   "metadata": {},
   "source": [
    "# Step 5: Determining the Number of Stations Recording Rainfall â‰¥ 1.00 mm on Extreme Rainfall Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cf506-d1b6-4eb1-b977-3a4b16533fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "excel_file_path = ''\n",
    "parquet_file_path = ''\n",
    "\n",
    "# Read the Excel file\n",
    "excel_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Ensure the 'Date' column in both files is in the same format (YYYY-MM-DD)\n",
    "excel_df['Date'] = pd.to_datetime(excel_df['Date'])\n",
    "parquet_df['Date'] = pd.to_datetime(parquet_df['Date'])\n",
    "\n",
    "# Initialize a list to store station counts for each date\n",
    "station_counts = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a list to store station counts for each date\n",
    "station_counts = []\n",
    "\n",
    "# Loop over each date in the Excel file and count the number of stations with Rainfall >= 1.00 mm using tqdm for progress tracking\n",
    "for date in tqdm(excel_df['Date'], desc=\"Processing Dates\"):\n",
    "    count = parquet_df[(parquet_df['Date'] == date) & (parquet_df['Rainfall'] >= 1.00)].shape[0]\n",
    "    station_counts.append(count)\n",
    "\n",
    "# Add the station counts to the Excel dataframe\n",
    "excel_df['Station Count'] = station_counts\n",
    "\n",
    "# Display the updated Excel dataframe\n",
    "excel_df.head()\n",
    "\n",
    "# Save the updated Excel dataframe with station counts back to the original Excel file\n",
    "output_file_path = excel_file_path  # Reusing the same path for saving\n",
    "\n",
    "# Save the updated dataframe back to the Excel file\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    excel_df.to_excel(writer, index=False)\n",
    "\n",
    "# Confirm that the file has been saved\n",
    "output_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef0851-0ba4-44f4-84bf-5dcb73baf2a3",
   "metadata": {},
   "source": [
    "# Step 6: Calculating the Active Station Count for Each Year with Extreme Rainfall Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9a6cd-558a-451b-b0fc-51dab0e9fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the station files\n",
    "directory = ''\n",
    "\n",
    "# Initialize a dictionary to store the count of valid stations per year\n",
    "yearly_station_count = {year: {'count': 0, 'filenames': []} for year in range(1858, 2024)}\n",
    "\n",
    "# Go through each CSV file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Ensure 'Year' is treated as integer\n",
    "        df['Year'] = pd.to_numeric(df['Year'], errors='coerce', downcast='integer')\n",
    "        \n",
    "        # Filter out rows where Rainfall is blank or negative, and quality of precipitation is 'N' for years > 2019\n",
    "        if 'Quality of precipitation value' in df.columns:\n",
    "            valid_rainfall = df[(df['Rainfall'].notnull()) & (df['Rainfall'] != -99.99) & \n",
    "                                 ((df['Year'] >= 2024) | (df['Quality of precipitation value'] != 'N'))]\n",
    "        else:\n",
    "            # If the column 'Quality_of_Precipitation' does not exist, just filter based on Rainfall\n",
    "            valid_rainfall = df[(df['Rainfall'].notnull()) & (df['Rainfall'] != -99.99)]\n",
    "        \n",
    "        # Group by 'Year' and process each year\n",
    "        for year, group in valid_rainfall.groupby('Year'):\n",
    "            if year in yearly_station_count:\n",
    "                total_days = len(df[df['Year'] == year])\n",
    "                \n",
    "                if total_days > 0 and len(group) / total_days > 0.1:\n",
    "                    yearly_station_count[year]['count'] += 1\n",
    "                    # Add filename without .csv extension\n",
    "                    yearly_station_count[year]['filenames'].append(filename[:-8])\n",
    "\n",
    "# Convert the result dictionary into a DataFrame\n",
    "data = []\n",
    "for year, info in yearly_station_count.items():\n",
    "    if info['count'] > 0:\n",
    "        data.append([year, info['count'], ',     '.join(info['filenames'])])\n",
    "\n",
    "df_yearly_count = pd.DataFrame(data, columns=['Year', 'Station_count', 'Filenames'])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    " excel_path = ''\n",
    " df_yearly_count.to_excel(excel_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478358a-4783-4252-9fcc-ea88e2a5f7a3",
   "metadata": {},
   "source": [
    "# Step 7: Assigning Wet to Active Station Ratio to each Extreme Rainfall Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee79d4-3fca-47a4-8860-7d22023ed2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "excel_file_path = ''\n",
    "\n",
    "# Read Sheet1 (ERD_dates) to get station counts\n",
    "sheet1_df = pd.read_excel(excel_file_path, sheet_name='Sheet1')  # Assuming Sheet1 is the first sheet\n",
    "\n",
    "# Read Sheet2 (Active Station Count)\n",
    "sheet2_df = pd.read_excel(excel_file_path, sheet_name='Sheet2')  # Assuming Sheet2 is the second sheet\n",
    "\n",
    "# Ensure the Year column in Sheet2 is in the same format as the date year in Sheet1\n",
    "sheet1_df['Year'] = pd.to_datetime(sheet1_df['Date']).dt.year\n",
    "\n",
    "# Merge the two dataframes on the Year\n",
    "merged_df = pd.merge(sheet1_df, sheet2_df, on='Year', how='left')\n",
    "\n",
    "# Calculate the Ratio and add it as a new column\n",
    "merged_df['Ratio'] = merged_df['Station Count'] / merged_df['Active_Station_count']\n",
    "\n",
    "\n",
    "# Save the updated Excel dataframe with station counts back to the original Excel file\n",
    "output_file_path = excel_file_path  # Reusing the same path for saving\n",
    "\n",
    "# Save the updated dataframe back to the Excel file\n",
    "with pd.ExcelWriter(output_file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    merged_df.to_excel(writer, index=False)\n",
    "\n",
    "# Confirm that the file has been saved\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913942e1-3591-476f-8bde-dc910db06f62",
   "metadata": {},
   "source": [
    "# Step 8A: Classification of Widespread vs Isolated Events\n",
    "Every Extreme Rainfall day based on their wet to active station ratio is either classified into Widespread Day and Isolated Day. This was done manually in the excel file with the help of filter function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3a40f-8236-4b59-8741-5ef22fff7b71",
   "metadata": {},
   "source": [
    "# Step 8B: Getting the list of Consecutive Widespread Day and Non Consecutive Widespread Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845a8b7-ae04-48b9-ac1a-149f8a50ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path =\"\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Read the 'Date' column from the specified sheet\n",
    "df = pd.read_excel(xls, sheet_name='Sheet1')\n",
    "\n",
    "# Ensure 'Date' is a datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Find non-consecutive dates\n",
    "non_consecutive_dates = []\n",
    "consecutive_dates = []\n",
    "\n",
    "for i in range(1, len(df) - 1):\n",
    "    if (df['Date'].dt.date[i] - df['Date'].dt.date[i - 1]).days > 1 and \\\n",
    "       (df['Date'].dt.date[i + 1] - df['Date'].dt.date[i]).days > 1:\n",
    "        non_consecutive_dates.append(df.iloc[i])\n",
    "    else:\n",
    "        consecutive_dates.append(df.iloc[i])\n",
    "\n",
    "# Create DataFrames from the lists\n",
    "non_consecutive_df = pd.DataFrame(non_consecutive_dates)\n",
    "consecutive_df = pd.DataFrame(consecutive_dates)\n",
    "\n",
    "# Format the 'Date' column to YYYY-MM-DD\n",
    "non_consecutive_df['Date'] = non_consecutive_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "consecutive_df['Date'] = consecutive_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Write the non-consecutive dates to Sheet2 and consecutive dates to Sheet3\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "    non_consecutive_df.to_excel(writer, sheet_name='Sheet3', index=False)\n",
    "    consecutive_df.to_excel(writer, sheet_name='Sheet4', index=False)\n",
    "\n",
    "print(\"Non-consecutive dates copied to Sheet2 and consecutive dates copied to Sheet3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ed7f7-cbac-438f-9fea-b4a755383ebe",
   "metadata": {},
   "source": [
    "# Step 8C: Grouping Consecutive Widespread Day into a single unique events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddb10f-7965-4639-a4d5-2ba7ada2aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file and Sheet 2\n",
    "file_path = ''\n",
    "sheet2 = pd.read_excel(file_path, sheet_name='')\n",
    "\n",
    "# Ensure 'Date' column is in 'YYYY-MM-DD' format\n",
    "sheet2['Date'] = pd.to_datetime(sheet2['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Sort the dataframe by Date\n",
    "sheet2 = sheet2.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "# Initialize the 'Event' column as empty strings\n",
    "sheet2['Event'] = ''\n",
    "\n",
    "# Assign event names\n",
    "event_count = 1\n",
    "sheet2.at[0, 'Event'] = f'Event_{event_count}'  # First event\n",
    "\n",
    "for i in range(1, len(sheet2)):\n",
    "    # Check if the current date is consecutive to the previous one\n",
    "    if pd.to_datetime(sheet2['Date'].iloc[i]) - pd.to_datetime(sheet2['Date'].iloc[i-1]) == pd.Timedelta(days=1):\n",
    "        # Assign same event for consecutive dates\n",
    "        sheet2.at[i, 'Event'] = f'Event_{event_count}'\n",
    "    else:\n",
    "        # New event for non-consecutive dates\n",
    "        event_count += 1\n",
    "        sheet2.at[i, 'Event'] = f'Event_{event_count}'\n",
    "\n",
    "# Save back to the same Excel file, updating only Sheet 2\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "    sheet2.to_excel(writer, sheet_name='', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bde58a-2b6a-46a8-b68b-e3905e3c0cf5",
   "metadata": {},
   "source": [
    "# Step 9A: Saving Each Unique Widespread Event Rainfall Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9948a70-43c3-48c8-ad24-d139f0f1b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "#This file contain unique widespread event identified in Step 8C\n",
    "excel_path = \"\"\n",
    "#This would be single combined parquet file containing all accepted rainfall records\n",
    "station_data = \"\"\n",
    "output_dir = ''\n",
    "\n",
    "# Load event data\n",
    "events_df = pd.read_excel(excel_path, sheet_name='Widespread_Event_Cluster')\n",
    "all_station_data = pd.read_parquet(station_data)\n",
    "\n",
    "# Group data by 'Event' and get all associated dates\n",
    "grouped_events = events_df.groupby('Event')\n",
    "\n",
    "# Loop over each event\n",
    "for event_name, group in tqdm(grouped_events, desc=\"Processing Events\"):\n",
    "    # Extract dates for the current event\n",
    "    event_dates = pd.to_datetime(group['Date'])  # Convert 'Date' column to datetime\n",
    "\n",
    "    # Filter the combined data for the date range\n",
    "    filtered_data = all_station_data[all_station_data['Date'].isin(event_dates)]\n",
    "    \n",
    "    # Store the rainfall data\n",
    "    rainfall_data = []\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        if row['Rainfall'] >= 0:  # Filter out negative rainfall values\n",
    "            rainfall_data.append([row['Station Number'], row['Date'].strftime('%Y-%m-%d'), row['Rainfall'], row['Lat'], row['Lon']])\n",
    "\n",
    "    # Convert rainfall data to a DataFrame\n",
    "    if rainfall_data:\n",
    "        rainfall_df = pd.DataFrame(rainfall_data, columns=['Station Number', 'Date', 'Rainfall', 'Latitude', 'Longitude'])\n",
    "        \n",
    "        # Pivot the DataFrame to have dates as columns\n",
    "        pivot_df = rainfall_df.pivot_table(index=['Station Number', 'Latitude', 'Longitude'], columns='Date', values='Rainfall', fill_value=0)\n",
    "        \n",
    "        # Reset index to make 'Station Number', 'Latitude', 'Longitude' columns again\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        event_filename = f\"{event_name}.parquet\"\n",
    "        output_file_path = os.path.join(output_dir, event_filename)\n",
    "        pivot_df.to_parquet(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Saved data for {event_name} to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc8a0c-e596-4876-8db7-a0a429e2a270",
   "metadata": {},
   "source": [
    "# Step 9B: Visualizing Rainfall Dot Maps for Each Identified Widespread Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b58e5-6888-4692-b0c3-8eed1be493d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Load the parquet file\n",
    "\n",
    "#Single file containing all of your accepted rainfall records\n",
    "parquet_file_path = '/combined_parquet.parquet'\n",
    "\n",
    "#Containing the rainfall information for a particular widespread event\n",
    "station_df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "#Single file containing all of accepted extreme rainfall records \n",
    "extreme_parquet_file_path = '/combined_extreme_parquet.parquet'\n",
    "\n",
    "extreme_station_df = pd.read_parquet(extreme_parquet_file_path)\n",
    "\n",
    "# Load the GeoJSON file for the map\n",
    "geojson_file_path = ''\n",
    "gdf_map = gpd.read_file(geojson_file_path)\n",
    "\n",
    "# Separate geometries (polygons and lines)\n",
    "gdf_polygons = gdf_map[gdf_map.geometry.type == 'Polygon']\n",
    "gdf_lines = gdf_map[gdf_map.geometry.type == 'LineString']\n",
    "\n",
    "# Create a custom colormap from white (0 mm), blue (medium), to red (>100 mm)\n",
    "colors = [(1, 1, 1), (0, 0, 1), (1, 0, 0)]  # white -> blue -> red\n",
    "rainfall_cmap = LinearSegmentedColormap.from_list(\"WhiteBlueRed\", colors)\n",
    "\n",
    "# User input prompt for specific date\n",
    "specific_date_str = input(\"Enter the date (YYYY-MM-DD) for which you want to generate the Rainfall dot map: \")\n",
    "specific_date = pd.to_datetime(specific_date_str)\n",
    "\n",
    "# Filter station data for the specific day\n",
    "daily_station_data = station_df[\n",
    "    (station_df['Date'] == specific_date) &\n",
    "    (station_df['Rainfall'] >= 0.00)  # Include all rainfall values\n",
    "]\n",
    "\n",
    "# Filter extreme station data for the same specific day\n",
    "extreme_station_data = extreme_station_df[\n",
    "    (extreme_station_df['Date'] == specific_date) &\n",
    "    (extreme_station_df['Rainfall'] >= 0.00)  # Include all rainfall values\n",
    "]\n",
    "\n",
    "# Check if there is any data to plot\n",
    "if not daily_station_data.empty:\n",
    "    # Plot the base map with polygons and lines\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    gdf_polygons.plot(ax=ax, color='grey', edgecolor='none', alpha=0.5)\n",
    "    gdf_lines.plot(ax=ax, color='black', alpha=1)\n",
    "\n",
    "    # Plot the station markers for general rainfall\n",
    "    scatter = ax.scatter(\n",
    "        daily_station_data['Lon'], \n",
    "        daily_station_data['Lat'], \n",
    "        c=daily_station_data['Rainfall'], \n",
    "        cmap=rainfall_cmap,  # Custom colormap\n",
    "        vmin=0, vmax=100,  # Rainfall from 0 to 100+\n",
    "        s=50, linewidth=0.5, zorder=2, edgecolor='k'\n",
    "    )\n",
    "\n",
    "    # Highlight the extreme stations with a thicker black border\n",
    "    ax.scatter(\n",
    "        extreme_station_data['Lon'], \n",
    "        extreme_station_data['Lat'], \n",
    "        facecolor='none',  # Keep facecolor empty to avoid filling\n",
    "        edgecolor='black',  # Black border for extreme rainfall\n",
    "        s=100, linewidth=2, zorder=4  # Thicker black edge with larger size\n",
    "    )\n",
    "\n",
    "    # Add color bar for rainfall\n",
    "    cbar = plt.colorbar(scatter, ax=ax,pad=0.02)\n",
    "    cbar.set_label('Rainfall (mm)',fontsize=15)\n",
    "    \n",
    "    # Title for the plot\n",
    "    plt.title(f\"Rainfall on {specific_date.strftime('%Y-%m-%d')}\", fontsize=20)\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Longitude', fontsize=15)\n",
    "    plt.ylabel('Latitude', fontsize=15)\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the image\n",
    "    output_dir = ''\n",
    "    output_file = f\"{output_dir}/1_Rainfall_Extreme_{specific_date.strftime('%Y_%m_%d')}.png\"\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No rainfall data available for {specific_date.strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c74f9c-912e-45d3-a01f-ef69829619d0",
   "metadata": {},
   "source": [
    "# Step 10: Applying Adaptive Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392062b-cf07-4bf8-9d6e-ebdac95728dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "\n",
    "#Directory containing the rainfall information for each unique widespread event\n",
    "parquet_dir = ''\n",
    "\n",
    "#File for NSW Map with GDR boundaries highlighted\n",
    "geojson_file_path = ''\n",
    "\n",
    "#Output directory to visually see the Extreme Rainfall patterns identified for Each Unique Widespread Event\n",
    "output_dir = ''\n",
    "\n",
    "#File for saving the lat and lon coordinates for every identified highest KDE density peak\n",
    "csv_output_file = ''\n",
    "\n",
    "# Read the GeoJSON file\n",
    "gdf_map = gpd.read_file(geojson_file_path)\n",
    "\n",
    "# Get the bounding box of the map geometries\n",
    "min_x, min_y, max_x, max_y = gdf_map.total_bounds\n",
    "\n",
    "# Get a sorted list of Parquet files in the directory\n",
    "parquet_files = sorted([f for f in os.listdir(parquet_dir) if f.endswith(\".parquet\")])\n",
    "\n",
    "# Filter files for events 1 to 3194\n",
    "event_files = [\n",
    "    f for f in parquet_files \n",
    "    if f.startswith(\"Event_\") and f[6:-8].isdigit() and 1895 <= int(f[6:-8]) <= 2000\n",
    "]\n",
    "\n",
    "# Prepare DataFrame to store joined line string coordinates\n",
    "linestring_df = pd.DataFrame(columns=['Event', 'Longitude', 'Latitude', 'Date'])\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(filename='event_processing.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def process_event_file(file):\n",
    "    try:\n",
    "        # Check if the image for the event is already generated\n",
    "        output_file = os.path.join(output_dir, f'{file[:-8]}.png')\n",
    "        if os.path.exists(output_file):\n",
    "            return f\"Skipped {file}, image already exists.\"\n",
    "\n",
    "        parquet_file_path = os.path.join(parquet_dir, file)\n",
    "        df = pq.read_table(parquet_file_path).to_pandas()\n",
    "\n",
    "        # Replace blank values and -99.9 with NaN\n",
    "        df.replace([-99.9, '', None], np.nan, inplace=True)\n",
    "\n",
    "        # Convert columns to numeric\n",
    "        for col in df.columns[3:]:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        centroids = []\n",
    "\n",
    "        # Loop through each date column\n",
    "        for date in df.columns[3:]:\n",
    "            valid_data = df[['Latitude', 'Longitude', date]].dropna()\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "\n",
    "            valid_data = valid_data[valid_data[date] > 0.001]\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "\n",
    "            latitudes = valid_data['Latitude']\n",
    "            longitudes = valid_data['Longitude']\n",
    "            rainfall = valid_data[date]\n",
    "            coords = np.vstack([latitudes, longitudes]).T\n",
    "\n",
    "            # Initial KDE fitting using rainfall as weights\n",
    "            kde = KernelDensity(bandwidth=1.5, kernel='gaussian')\n",
    "            kde.fit(coords, sample_weight=rainfall)\n",
    "            log_density = kde.score_samples(coords)\n",
    "            density = np.exp(log_density) + 1e-10\n",
    "            bandwidths = 1.0 / np.sqrt(density)\n",
    "\n",
    "            #Applying Adaptive KDE\n",
    "            adaptive_kde_vals = np.zeros((100, 100))\n",
    "            grid_lat, grid_lon = np.mgrid[min_y:max_y:100j, min_x:max_x:100j]\n",
    "            grid_points = np.vstack([grid_lat.ravel(), grid_lon.ravel()]).T\n",
    "\n",
    "            for i, bw in enumerate(bandwidths):\n",
    "                kde_adaptive = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "                kde_adaptive.fit(coords[i:i+1], sample_weight=[rainfall.iloc[i]])\n",
    "                kde_scores = kde_adaptive.score_samples(grid_points)\n",
    "                adaptive_kde_vals += np.exp(kde_scores).reshape(100, 100)\n",
    "\n",
    "            #Masking the points which are outside of the map\n",
    "            mask = np.zeros(adaptive_kde_vals.shape, dtype=bool)\n",
    "            for geom in gdf_map.geometry:\n",
    "                if geom.is_valid:\n",
    "                    points_in_geom = gpd.points_from_xy(grid_lon.ravel(), grid_lat.ravel())\n",
    "                    points_gdf = gpd.GeoDataFrame(geometry=points_in_geom, crs=gdf_map.crs)\n",
    "                    mask |= points_gdf.within(geom).values.reshape(adaptive_kde_vals.shape)\n",
    "\n",
    "            adaptive_kde_vals[~mask] = np.nan\n",
    "\n",
    "            #Filtering out the values where KDE values are above 95th percentile threshold \n",
    "            kde_95_threshold = np.nanpercentile(adaptive_kde_vals, 95)\n",
    "            high_density_mask = adaptive_kde_vals >= kde_95_threshold\n",
    "\n",
    "            \n",
    "            high_density_labels, num_features = label(high_density_mask)\n",
    "            centroids_of_areas = center_of_mass(adaptive_kde_vals, high_density_labels, range(1, num_features + 1))\n",
    "\n",
    "            #Getting the highest peak and its corresponding lat and lon information\n",
    "            if centroids_of_areas:\n",
    "                largest_centroid = max(centroids_of_areas, key=lambda c: adaptive_kde_vals[int(c[0]), int(c[1])])\n",
    "                lat_centroid, lon_centroid = grid_lat[int(largest_centroid[0]), 0], grid_lon[0, int(largest_centroid[1])]\n",
    "                centroids.append((lat_centroid, lon_centroid, date))\n",
    "\n",
    "        if centroids:\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            gdf_map[gdf_map.geometry.type == 'Polygon'].plot(ax=ax, color='grey', edgecolor='none', alpha=0.5)\n",
    "            gdf_map[gdf_map.geometry.type == 'LineString'].plot(ax=ax, color='black', alpha=0.5)\n",
    "\n",
    "            lons = [lon for lat, lon, date in centroids]\n",
    "            lats = [lat for lat, lon, date in centroids]\n",
    "            dates = [date for lat, lon, date in centroids]\n",
    "\n",
    "            ax.plot(lons, lats, color='red', linestyle='-', marker='o')\n",
    "            for lon, lat, date in zip(lons, lats, dates):\n",
    "                ax.annotate(date, (lon, lat), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8, color='red')\n",
    "\n",
    "            new_rows = []\n",
    "            event_name = os.path.splitext(file)[0]\n",
    "            for lon, lat, date in zip(lons, lats, dates):\n",
    "                new_rows.append({'Event': event_name, 'Longitude': lon, 'Latitude': lat, 'Date': date})\n",
    "\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "            # Append new data to CSV\n",
    "            new_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "\n",
    "            ax.set_title(f'{event_name}', fontsize=16)\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "\n",
    "            plt.savefig(output_file, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        # Track successful processing\n",
    "        logging.info(f\"Successfully processed {file}\")\n",
    "        gc.collect()  # Manually trigger garbage collection\n",
    "        return f\"Processed {file}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file}: {e}\")\n",
    "        return f\"Error processing {file}\"\n",
    "\n",
    "# Parallel processing with error handling\n",
    "results = Parallel(n_jobs=56, timeout=1000000)(\n",
    "    delayed(process_event_file)(file) for file in tqdm(event_files, desc=\"Processing Parquet files\")\n",
    ")\n",
    "\n",
    "# After processing all files\n",
    "logging.info(f\"Processing completed. Line string coordinates saved incrementally to {csv_output_file}\")\n",
    "print(f\"Line string coordinates saved incrementally to {csv_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc1e1b-dc89-47ef-b056-bbe056bb815f",
   "metadata": {},
   "source": [
    "# Step 11: Identifying and removing Coastal Widespread Extreme Rainfall Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcf275-c0dc-41ae-bb48-f26a08a6b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the  coastal highlighted polygons  from the GeoJSON file\n",
    "highlighted_polygons_path = '/highlighted_polygons.geojson'\n",
    "highlighted_polygons = gpd.read_file(highlighted_polygons_path)\n",
    "\n",
    "# Load the file which contain the information about lat lon coordinates for every identified highest KDE peak in each unique widespread\n",
    "# extreme rainfall event data from the Excel file\n",
    "joined_linestring_path = ''\n",
    "joined_linestring_df_sheet2 = pd.read_excel(joined_linestring_path, sheet_name=\"IS\")\n",
    "\n",
    "# Load all the event data (as a GeoDataFrame)\n",
    "joined_linestring_df = pd.read_excel(joined_linestring_path)\n",
    "geometry = gpd.points_from_xy(joined_linestring_df['Lon'], joined_linestring_df['Lat'])\n",
    "joined_linestring_gdf = gpd.GeoDataFrame(joined_linestring_df, geometry=geometry)\n",
    "\n",
    "# List to store coastal events\n",
    "coastal_events = []\n",
    "\n",
    "# Check each event\n",
    "for event in joined_linestring_gdf['Event'].unique():\n",
    "    # Filter data for the current event\n",
    "    event_data = joined_linestring_gdf[joined_linestring_gdf['Event'] == event]\n",
    "    \n",
    "    # Check if all points are within any highlighted polygons\n",
    "    all_within = event_data.geometry.apply(lambda point: highlighted_polygons.contains(point).any()).all()\n",
    "    \n",
    "    # If all points are within the polygons, it's a coastal event\n",
    "    if all_within:\n",
    "        print(f\"{event} is a Coastal Event\")\n",
    "        coastal_events.append(event)\n",
    "\n",
    "# Remove rows in Sheet2 that belong to coastal events\n",
    "updated_df_sheet2 = joined_linestring_df_sheet2[~joined_linestring_df_sheet2['Event'].isin(coastal_events)]\n",
    "\n",
    "# Separate the coastal events data to save in \"CS\" sheet\n",
    "coastal_events_df = joined_linestring_df_sheet2[joined_linestring_df_sheet2['Event'].isin(coastal_events)]\n",
    "\n",
    "# Save the updated DataFrame and coastal events DataFrame back to the Excel file\n",
    "with pd.ExcelWriter(joined_linestring_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "    updated_df_sheet2.to_excel(writer, sheet_name=\"IS\", index=False)  # Update Sheet2\n",
    "    coastal_events_df.to_excel(writer, sheet_name=\"ICS\", index=False)  # Save coastal events in \"CS\"\n",
    "\n",
    "print(f\"Removed rows corresponding to the following coastal events from Sheet2 and saved them in the 'CS' sheet: {coastal_events}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1734f-e5fb-4303-98ad-f5c5eeb0d828",
   "metadata": {},
   "source": [
    "# Step 12: Widespread Event Extreme Rainfall Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fb08c-0cbe-4d94-bfc4-11492e49c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "# File paths containing the information about lat lon coordinates for every identified highest KDE peak in each unique widespread extreme rainfall event data from the Excel file\n",
    "excel_file_path = ''\n",
    "geojson_file_path = ''\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_file_path,sheet_name='')\n",
    "\n",
    "# Filter for Event_X\n",
    "event_1_data = df[df['Event'] == '']\n",
    "\n",
    "# Create GeoDataFrame for centroids\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(event_1_data['Lon'], event_1_data['Lat'])]\n",
    "gdf_event_1 = gpd.GeoDataFrame(event_1_data, geometry=geometry)\n",
    "\n",
    "# Calculate the centroid of the points\n",
    "centroid = gdf_event_1.geometry.unary_union.centroid\n",
    "\n",
    "# Read the GeoJSON file\n",
    "gdf_map = gpd.read_file(geojson_file_path)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the map\n",
    "gdf_map[gdf_map.geometry.type == 'Polygon'].plot(ax=ax, color='grey', edgecolor='none', alpha=0.5)\n",
    "gdf_map[gdf_map.geometry.type == 'LineString'].plot(ax=ax, color='black', alpha=0.5)\n",
    "\n",
    "# Plot the centroids\n",
    "gdf_event_1.plot(ax=ax, color='blue', markersize=50, label='Centroids')\n",
    "\n",
    "# Annotate the centroids with their respective dates\n",
    "for idx, row in gdf_event_1.iterrows():\n",
    "    ax.annotate(row['Date'], xy=(row['Lon'], row['Lat']),\n",
    "                xytext=(3, 3), textcoords=\"offset points\", fontsize=10, color='black')\n",
    "\n",
    "# Draw lines between the centroids\n",
    "if len(gdf_event_1) > 1:\n",
    "    line = LineString(gdf_event_1.geometry.tolist())\n",
    "    ax.plot(*line.xy, color='blue', linewidth=2)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Storm of 1885-01-22 to 1885-01-27', fontsize=20)\n",
    "ax.set_xlabel('Longitude', fontsize=15)\n",
    "ax.set_ylabel('Latitude', fontsize=15)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a8426-b419-46f5-90a0-51ffdd57bfd4",
   "metadata": {},
   "source": [
    "# Step 13A: Removing Random 50 stations from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126ad12-0116-4fac-bed5-59af84ea267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import openpyxl\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.ndimage import measurements\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Directory containing the rainfall information for each identified unique widespread extreme rainfall event\n",
    "base_parquet_dir = ''\n",
    "geojson_file_path = ''\n",
    "#File for saving the lat lon coordinates of KDE peak after removing the random 50 stations from the network\n",
    "output_excel_path = ''\n",
    "\n",
    "# List of events to analyze\n",
    "event_list = []\n",
    "\n",
    "# Read the GeoJSON file\n",
    "gdf_map = gpd.read_file(geojson_file_path)\n",
    "\n",
    "# Function to process a single event\n",
    "def process_event(event_name, gdf_map, output_excel_path):\n",
    "    parquet_file_path = os.path.join(base_parquet_dir, f\"{event_name}.parquet\")\n",
    "    if not os.path.exists(parquet_file_path):\n",
    "        print(f\"File not found: {parquet_file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    df.replace([-99.9, '', None], np.nan, inplace=True)\n",
    "    for col in df.columns[3:]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    min_x, min_y, max_x, max_y = gdf_map.total_bounds\n",
    "    stations_step = 50\n",
    "    remaining_stations = len(df)\n",
    "    all_centroid_tracks = []\n",
    "\n",
    "    for step in range(200):\n",
    "        if remaining_stations <= 0:\n",
    "            break\n",
    "\n",
    "        sampled_df = df.sample(n=remaining_stations, random_state=42)\n",
    "        centroids = []\n",
    "\n",
    "        for date in sampled_df.columns[3:]:\n",
    "            valid_data = sampled_df[['Latitude', 'Longitude', date]].dropna()\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "\n",
    "            valid_data = valid_data[valid_data[date] > 0.001]\n",
    "            if valid_data.empty:\n",
    "                continue\n",
    "\n",
    "            coords = np.vstack([valid_data['Latitude'], valid_data['Longitude']]).T\n",
    "            rainfall = valid_data[date]\n",
    "            kde = KernelDensity(bandwidth=1.5, kernel='gaussian')\n",
    "            kde.fit(coords, sample_weight=rainfall)\n",
    "            log_density = kde.score_samples(coords)\n",
    "            density = np.exp(log_density) + 1e-10\n",
    "\n",
    "            bandwidths = 1.0 / np.sqrt(density)\n",
    "            adaptive_kde_vals = np.zeros((100, 100))\n",
    "            grid_lat, grid_lon = np.mgrid[min_y:max_y:100j, min_x:max_x:100j]\n",
    "            grid_points = np.vstack([grid_lat.ravel(), grid_lon.ravel()]).T\n",
    "\n",
    "            for i, bw in enumerate(bandwidths):\n",
    "                kde_adaptive = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "                kde_adaptive.fit(coords[i:i+1], sample_weight=[rainfall.iloc[i]])\n",
    "                kde_scores = kde_adaptive.score_samples(grid_points)\n",
    "                adaptive_kde_vals += np.exp(kde_scores).reshape(100, 100)\n",
    "\n",
    "            mask = np.zeros(adaptive_kde_vals.shape, dtype=bool)\n",
    "            for geom in gdf_map.geometry:\n",
    "                if geom.is_valid:\n",
    "                    points_in_geom = gpd.points_from_xy(grid_lon.ravel(), grid_lat.ravel())\n",
    "                    points_gdf = gpd.GeoDataFrame(geometry=points_in_geom, crs=gdf_map.crs)\n",
    "                    mask |= points_gdf.within(geom).values.reshape(adaptive_kde_vals.shape)\n",
    "\n",
    "            adaptive_kde_vals[~mask] = np.nan\n",
    "            kde_95_threshold = np.nanpercentile(adaptive_kde_vals, 95)\n",
    "            high_density_mask = adaptive_kde_vals >= kde_95_threshold\n",
    "            high_density_labels, num_features = measurements.label(high_density_mask)\n",
    "            centroids_of_areas = measurements.center_of_mass(adaptive_kde_vals, high_density_labels, range(1, num_features + 1))\n",
    "\n",
    "            if centroids_of_areas:\n",
    "                largest_centroid = max(centroids_of_areas, key=lambda c: adaptive_kde_vals[int(c[0]), int(c[1])])\n",
    "                lat_centroid, lon_centroid = grid_lat[int(largest_centroid[0]), 0], grid_lon[0, int(largest_centroid[1])]\n",
    "                centroids.append((lat_centroid, lon_centroid, date))\n",
    "\n",
    "        all_centroid_tracks.append(centroids)\n",
    "        remaining_stations -= stations_step\n",
    "\n",
    "    centroids_data = []\n",
    "    for step, centroids in enumerate(all_centroid_tracks):\n",
    "        for lat, lon, date in centroids:\n",
    "            centroids_data.append({\n",
    "                'Filename': event_name,\n",
    "                'Date': date,\n",
    "                'Step': step * stations_step,\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon\n",
    "            })\n",
    "\n",
    "    centroids_df = pd.DataFrame(centroids_data)\n",
    "    centroids_pivot = centroids_df.pivot(index=['Filename', 'Date'], columns='Step', values=['Latitude', 'Longitude'])\n",
    "    centroids_pivot.columns = ['_'.join(map(str, col)) for col in centroids_pivot.columns]\n",
    "    centroids_pivot.reset_index(inplace=True)\n",
    "\n",
    "    if os.path.exists(output_excel_path):\n",
    "        existing_df = pd.read_excel(output_excel_path, sheet_name='Centroids')\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    updated_df = pd.concat([existing_df, centroids_pivot], ignore_index=True)\n",
    "    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "        updated_df.to_excel(writer, index=False, sheet_name='Centroids')\n",
    "\n",
    "    print(f\"Centroid data for {event_name} appended to {output_excel_path}\")\n",
    "\n",
    "for event in tqdm(event_list, desc=\"Processing events\"):\n",
    "    process_event(event, gdf_map, output_excel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d0649-7057-4af6-8678-e7140f5443a6",
   "metadata": {},
   "source": [
    "# Step 13B: To Plot extreme rainfall trajectories in subplots at each step removal of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef80286-4229-4070-a143-b29a3bfa9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# File paths containing the information about lat lon coordinates for every identified highest KDE peak in each unique widespread extreme rainfall event data from the Excel file\n",
    "excel_file_path = ''\n",
    "geojson_file_path = ''\n",
    "#Directory containing the rainfall information for each identified unique widespread extreme rainfall event\n",
    "parquet_dir = ''\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Read the GeoJSON file\n",
    "gdf_map = gpd.read_file(geojson_file_path)\n",
    "\n",
    "# Ask the user to select an event (for example, \"Event_303\")\n",
    "selected_event = input(\"Enter the event you want to visualize (e.g., 'Event_303'): \")\n",
    "\n",
    "# Filter the DataFrame based on the selected event\n",
    "df_selected_event = df[df['Filename'] == selected_event]\n",
    "\n",
    "# Extract steps from the columns (e.g., 0, 50, 100, etc.)\n",
    "steps = [int(col.split('_')[-1]) for col in df_selected_event.columns if col.startswith('Latitude_')]\n",
    "\n",
    "# Find the corresponding parquet file for the selected event\n",
    "parquet_file_path = os.path.join(parquet_dir, f'{selected_event}.parquet')\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_df = pd.read_parquet(parquet_file_path)\n",
    "total_stations = len(parquet_df)\n",
    "\n",
    "# Calculate total rainfall for each station\n",
    "rainfall_columns = parquet_df.columns[3:]  # Assuming rainfall data starts from the 4th column\n",
    "total_rainfall = parquet_df[rainfall_columns].sum(axis=1)\n",
    "\n",
    "# Add total rainfall to the DataFrame\n",
    "parquet_df['Total_Rainfall'] = total_rainfall\n",
    "\n",
    "# Number of rows and columns in subplots\n",
    "n_cols = 4\n",
    "n_rows = -(-len(steps) // n_cols)  # Ceiling division to calculate rows needed\n",
    "\n",
    "# Create a color map for rainfall values\n",
    "cmap = LinearSegmentedColormap.from_list(\"rainfall_cmap\", [\"white\", \"blue\", \"red\"])\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "# Flatten axes for easy indexing (handles case when n_cols > 1)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, step in enumerate(steps):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Extract latitude and longitude columns for the current step\n",
    "    lat_col = f'Latitude_{step}'\n",
    "    lon_col = f'Longitude_{step}'\n",
    "\n",
    "    # Check if the latitude and longitude columns contain valid data (i.e., not NaN)\n",
    "    if df_selected_event[lat_col].notna().any() and df_selected_event[lon_col].notna().any():\n",
    "        # Filter data for current step\n",
    "        step_latitudes = df_selected_event[lat_col]\n",
    "        step_longitudes = df_selected_event[lon_col]\n",
    "\n",
    "        # Plot the GeoJSON map\n",
    "        gdf_map[gdf_map.geometry.type == 'Polygon'].plot(ax=ax, color='grey', edgecolor='none', alpha=0.8)\n",
    "        gdf_map[gdf_map.geometry.type == 'LineString'].plot(ax=ax, color='black', linestyle='--',alpha=1,label='GDR boundary')\n",
    "\n",
    "        # Plot red dots and lines for the step\n",
    "        ax.scatter(step_longitudes, step_latitudes, color='#483D8B',s=80,edgecolor='black')\n",
    "        ax.plot(step_longitudes, step_latitudes, color='#483D8B', linestyle='-', alpha=0.8,linewidth=4.5)\n",
    "\n",
    "        # Shuffle the DataFrame for randomness\n",
    "        shuffled_parquet_df = parquet_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # Filter stations not removed in this step\n",
    "        remaining_station_count = total_stations - step\n",
    "        active_stations =  shuffled_parquet_df.iloc[:remaining_station_count]\n",
    "\n",
    "        # Plot stations with rainfall values\n",
    "        sc = ax.scatter(\n",
    "            active_stations['Longitude'], \n",
    "            active_stations['Latitude'], \n",
    "            c=active_stations['Total_Rainfall'], \n",
    "            cmap=cmap, \n",
    "            edgecolor='none', \n",
    "            s=30,  # Marker size\n",
    "            alpha=0.3\n",
    "        )\n",
    "\n",
    "        # Store the scatter plot for the colorbar\n",
    "        scatter_for_colorbar = sc\n",
    "        \n",
    "       # Add title and labels with remaining stations\n",
    "        ax.set_title(f'Remaining Stations: {remaining_station_count}', fontsize=15)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "    else:\n",
    "        # If no valid data for this step, hide the axis\n",
    "        ax.axis('off')\n",
    "\n",
    "# Hide any extra empty subplots\n",
    "for j in range(len(steps), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Add a common colorbar next to the bottom row of subplots\n",
    "if scatter_for_colorbar:\n",
    "    cbar_ax = fig.add_axes([0.92, 0.5, 0.02, 0.4])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(scatter_for_colorbar, cax=cbar_ax)\n",
    "    cbar.set_label('Total Rainfall (mm)', fontsize=12)\n",
    "\n",
    "# Add a suptitle for the entire figure\n",
    "fig.suptitle(f\"KDE Extreme Rainfall Track with Stepwise Station Removal\", fontsize=20, y=0.95)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Leave space for suptitle\n",
    "\n",
    "# Save the plot\n",
    "output_path = f'/{selected_event}_Centroid_Tracking.png'\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f907c49-a129-4fe1-ba01-09cbef275c69",
   "metadata": {},
   "source": [
    "# Final Step 14: Applying the Dynamic Time Warping Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e33361-8dc0-4607-b033-e5ceeec0a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "from kneed import KneeLocator\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# File paths\n",
    "excel_file = '/Users/coolkarni/Documents/Documents/Documents/Master thesis Data 3 /Centroids_each_day.xlsx'\n",
    "base_parquet_dir = '/Users/coolkarni/Documents/Documents/Documents/Master thesis Data 3 /19th_Century_Event_Files_parquet_network/'\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Group by Filename (unique events)\n",
    "grouped = df.groupby('Filename')\n",
    "\n",
    "# Initialize storage for DTW results\n",
    "dtw_results_all = {}\n",
    "elbow_points = []\n",
    "\n",
    "# Process each unique event\n",
    "for event, data in grouped:\n",
    "    base_lat = data['Latitude_0'].values\n",
    "    base_lon = data['Longitude_0'].values\n",
    "    \n",
    "    base_track = list(zip(base_lat, base_lon))\n",
    "    dtw_results = []\n",
    "    remaining_stations = []\n",
    "    \n",
    "    # Get total stations from corresponding parquet file\n",
    "    parquet_file = os.path.join(base_parquet_dir, f\"{event}.parquet\")\n",
    "    if not os.path.exists(parquet_file):\n",
    "        print(f\"Parquet file for event {event} not found.\")\n",
    "        continue\n",
    "    \n",
    "    total_stations = pq.read_table(parquet_file).shape[0]\n",
    "    \n",
    "    # Compare base track with other tracks\n",
    "    for col in range(1, len(data.columns) // 2):  # Assuming Latitude_x and Longitude_x pairs\n",
    "        lat_col = f\"Latitude_{col * 50}\"\n",
    "        lon_col = f\"Longitude_{col * 50}\"\n",
    "        \n",
    "        if lat_col not in data.columns or lon_col not in data.columns:\n",
    "            continue\n",
    "        \n",
    "        current_lat = data[lat_col].dropna().values\n",
    "        current_lon = data[lon_col].dropna().values\n",
    "        current_track = list(zip(current_lat, current_lon))\n",
    "        \n",
    "        # Skip if the current track is empty\n",
    "        if not current_track:\n",
    "            continue\n",
    "    \n",
    "        # Calculate DTW distance\n",
    "        try:\n",
    "            distance, _ = fastdtw(base_track, current_track, dist=euclidean)\n",
    "            dtw_results.append(distance)\n",
    "            remaining_stations.append(total_stations - col * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating DTW for {lat_col} and {lon_col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Store results for the event if there are valid DTW distances\n",
    "    if dtw_results:\n",
    "        dtw_results_all[event] = (remaining_stations, dtw_results)\n",
    "        \n",
    "        # Apply knee point detection for individual events\n",
    "        if len(dtw_results) > 2:\n",
    "            kl = KneeLocator(remaining_stations, dtw_results, curve='convex', direction='decreasing', online=True)\n",
    "            elbow_station = kl.knee\n",
    "            if elbow_station is not None:\n",
    "                elbow_points.append(elbow_station)\n",
    "\n",
    "# Calculate the mean of the elbow points from individual events\n",
    "mean_elbow_point = np.mean(elbow_points) if elbow_points else None\n",
    "round_mean_elbow_point = round(mean_elbow_point) if mean_elbow_point is not None else None\n",
    "\n",
    "# Gather DTW distances for each unique remaining station value across all events\n",
    "dtw_data_by_station = {}\n",
    "for event, (stations, distances) in dtw_results_all.items():\n",
    "    for station, distance in zip(stations, distances):\n",
    "        if station not in dtw_data_by_station:\n",
    "            dtw_data_by_station[station] = []\n",
    "        dtw_data_by_station[station].append(distance)\n",
    "\n",
    "# Sort remaining stations\n",
    "sorted_stations = sorted(dtw_data_by_station.keys())\n",
    "\n",
    "# Calculate mean and standard deviation for each remaining station\n",
    "mean_distances = []\n",
    "std_distances = []\n",
    "for station in sorted_stations:\n",
    "    values = dtw_data_by_station[station]\n",
    "    mean_distances.append(np.mean(values))\n",
    "    std_distances.append(np.std(values))\n",
    "\n",
    "# Apply Gaussian filter to smooth the mean and standard deviation\n",
    "sigma = 4  # Standard deviation for Gaussian kernel\n",
    "smoothed_mean_distances = gaussian_filter1d(mean_distances, sigma)\n",
    "smoothed_std_distances = gaussian_filter1d(std_distances, sigma)\n",
    "\n",
    "# Find the best elbow point with varying S values\n",
    "best_knee = None\n",
    "best_S = None\n",
    "for S in np.linspace(1, 1, 50):  # Try different S values (adjust range if needed)\n",
    "    try:\n",
    "        knee_locator = KneeLocator(sorted_stations, smoothed_mean_distances, curve=\"convex\", direction=\"decreasing\", S=S)\n",
    "        if knee_locator.knee:\n",
    "            best_knee = knee_locator.knee\n",
    "            best_S = S\n",
    "            break  # Stop at the first valid knee to optimize\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "print(f\"Best S value: {best_S}, Elbow at: {best_knee}\")\n",
    "\n",
    "# Create the combined plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot individual event trajectories in grey with low alpha\n",
    "for event, (stations, distances) in dtw_results_all.items():\n",
    "    plt.plot(stations, distances, marker='none', color='grey', alpha=0.09)\n",
    "\n",
    "# Plot the smoothed mean line in black\n",
    "plt.plot(sorted_stations, smoothed_mean_distances, color='black', linewidth=2, label='Mean DTW Distance')\n",
    "\n",
    "# Shade the area representing Â±1 smoothed standard deviation\n",
    "#plt.fill_between(sorted_stations,\n",
    "                 #smoothed_mean_distances - smoothed_std_distances,\n",
    "                 #smoothed_mean_distances + smoothed_std_distances,\n",
    "                 #color='grey', alpha=0.3, label='Â±1 Std Dev')\n",
    "\n",
    "# Highlight the best elbow point from smoothed data\n",
    "if best_knee is not None:\n",
    "    plt.scatter(best_knee, knee_locator.knee_y, color='red', s=100, label=f'Critical Station Threshold: {best_knee} Station', alpha=1)\n",
    "\n",
    "# Highlight the mean elbow point from individual events\n",
    "if mean_elbow_point is not None:\n",
    "    plt.axvline(x=mean_elbow_point, color='r', linestyle='--', label=f'Minimum Station Threshold: {round_mean_elbow_point} Station')\n",
    "\n",
    "# Customize y-axis ticks\n",
    "max_y = (max(smoothed_mean_distances) + max(smoothed_std_distances) if len(smoothed_mean_distances) > 0 and len(std_distances) > 0 else 0)\n",
    "y_ticks = np.arange(0, max_y + 4, 4)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "plt.xlabel('Remaining Stations', fontsize=15)\n",
    "plt.ylabel('DTW Distance', fontsize=15)\n",
    "plt.title('Trajectory Analysis', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
